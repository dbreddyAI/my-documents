{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply machine learning into 4 different area, seismic prediction model, \n",
    "use CNN to draw graph, to compress complex logical chain into vivid 2D graph. hidden state to memorize the ; \n",
    "it is in seattle, so I will use bruce lee word, says, be water my friend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> why use ML </h4>\n",
    "prediction, unknown x to predict y; inference, to understand relationship between x and y <br>\n",
    "\n",
    "<h4> parametric vs non-parametric </h4> \n",
    "parametric: assume function form, reduce complexity of problem; but if the form is far from the truth, it will perform poorly\n",
    "linear regression: assume weights are linear related to y <br> \n",
    "non-parametric: no explicitely assumption for function form, more flexible. <br>\n",
    "\n",
    "<h4> prediction accuracy vs. model interpretatbility </h4>\n",
    "more flexible model, higher prediction accuracy, lower model interpretatbility <br>\n",
    "subset, lasso, least square, additive model, tree, SVM, bagging, boosting. <br>\n",
    "\n",
    "<h4> bias-vairance trade off </h4>\n",
    "variance: change of coefficients of the model, when using different training data; \n",
    "bias: error introduced when approxiamate real-world problem; high bias occur when use low complexity model to approximate high-complexity problem. <br>\n",
    "total error = sum of bias + variance. by adjusting variance and bias to achieve a lowest total error <br>\n",
    "when change of hyper-p, the U shape of testing error <br> \n",
    "\n",
    "<h4> overfitting </h4>\n",
    "low training error, high testing error;  <br>\n",
    "avoid overfitting: shrikage method(subsets of features, regulization(cost funtion to regulate weights)), bagging or boosting <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Classification problem </h3>\n",
    "why not regression, because value of the outcome is not following order, so if using regression, different output label will give different regression model. <br> \n",
    "\n",
    "<h3> Bayes Classifier </h3>\n",
    "it produce the lowest test error rate, almost the irreducible error, <br> \n",
    "use the real conditional probability and choose largest $$ P(Y = j|X = x0) $$ \n",
    "decision boundary $ P(Y = j|X = x0)= 50\\% $ <br>\n",
    "drawback: don't know the conditional probability <br>\n",
    "\n",
    "<h3> k-nearest neighbor </h3>\n",
    "approximate the conditional probability  $$ P(Y = j|X) = \\frac{1}{K}\\sum{I(y_i=j)}   $$\n",
    "**algo**: calculate the distance between X_test and X_train, select the closest k, calculate the condition probability of each class $y_j$, divided by k served as normalization, select the highest probability one. <br> \n",
    "**hp**: higher k, lower flexibility;higher k produce more linear decision boundary as average more points; high k has higher bias and lower vairance <br>\n",
    "as $1/k$ increase, training decline, testing decline and increase. <br>\n",
    "\n",
    "<h3> k-nearest neighbor regression </h3>\n",
    "$$ f(x_0) =  \\frac{1}{K}\\sum{y_i} $$\n",
    "parametric: **good** reduce the complexity of problem to some parameters, easy to interpretate; easy to do test statistic; **bad** strong assumption, if not fit the assumption, have bad performance; <br>\n",
    "**when to use linear regression not KNN** linear-relation, or very high dimension, or want to interpretaion; use linear-regression; <br>\n",
    "\n",
    "**curse of dimension**: for high-dimension(high P), k-nearest neighbor perform worse then linear-regression, because cannot find neighbor, as the high dimension reduce the sample size. also when they are close in low dimension, but now they are far away <br>\n",
    "for interpretation purpose, perfer linear-regression <br> \n",
    "advantage: when decision boundary is non-linear <br> \n",
    "\n",
    "<h3> regression model </h3>\n",
    "\n",
    "**extend**: remove additive assumption ( additive is x1 effect is indepently to x2 effect on y1); \n",
    "use $ Y = \\beta_1X_1 + \\beta_2 X_1 X_2$ <br>\n",
    "add non-linear relaitonship: $ Y = \\beta_1X_1 + \\beta_2 X_1^2 $ <br>\n",
    "\n",
    "<h3> problem of linear model </h3>\n",
    "\n",
    "1. non-linearity of data, use residual plots, if no pattern show, then it is ok to use linear model,\n",
    "   if there is a pattern, first transform $x$ into $\\log{x}$, $\\sqrt{x}$\n",
    "   \n",
    "2. correlation of error term, if they have correlation, then estimated stanrdard error will **underestimate** the error, then condifence interval is wrong;   e.g. use the same data and make it double. the confidence  interval will decrease; it happend in time series data **solution**, plot residual as a fucntion of time  \n",
    "\n",
    "3. non-constant variance of error \n",
    "\n",
    "4. outlier, a point whose $y_i$ is far from the value predicted by the model; **detection** plot studentized residual \n",
    "\n",
    "5. high leverage points, a point whose $ x_i $ is far from the rest of x. **detection** compute leverage statistics, a large value indicate a high leverage point \n",
    "\n",
    "6. Collinearity: two or more predictor closedly related to each other. it increase the difficulty of getting lowest RSS region,convert the RSS contours into narrow valley. increase the uncertainty of coefficient estimating. \n",
    "**detection**, correlation matrix(only for pair collinerity), plot **variance inflation factor** (VIF for multicollinearity). \n",
    "**solution** drop out one, or combine the collinear vairables into single predictor. <br>\n",
    "\n",
    "<h3>VIF</h3>\n",
    "The covariance matrix, element(i,j) is the covariance of xi and xj. High covariance(>75%) needed to be removed. However, this **pair-wise** correlation is limited <br> \n",
    "VIF: quantifies how much the variance is inflated.<br>\n",
    "expressed $x_i$ in term of other predictors, calculate the $R_i^2$, the $VIF_i =  \\frac{1}{1-R_i^2}$; If $x_i$ can be linearly explained by other variable, its $R_i^2$ is high, so that $VIF_i$ is high; e.g if VIF>10 <br> \n",
    "**tolerance**: recepical of VIF.  \n",
    "\n",
    "<h3> compared knn with regression</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2 classification </h2>\n",
    "Why not apply regression to classification, different code the item into number will produce different model for same problem. <br>\n",
    "one exception is binary classification, which can still use regression method <br>\n",
    "\n",
    "<h3> logistic regression </h3>\n",
    "modeling conditional probability <br>\n",
    "**def**: \n",
    "linear regression + activation function $$ f = w * x ,  p(x) = \\frac{1}{1+e^{-f(x)}}$$\n",
    "odds function $P(x) / [1 -p(x) ] = e^{f(x)} = e^{wx} $, 0 mean low probability, inf mean high probability; log of odds function is linear of x; one unit increase of slope, is one unit increase of log odds function <br>\n",
    "**algo**: \n",
    "estimate the coeffs using (not using non-linear regression) but use **maximum likelihood** to estimate the\n",
    "conditional probability \n",
    "P is the probability of xi that make yi classified as 1, so max probability is P for y=1 max, and is (1-P) for y=0 max <br> \n",
    "$$ p(x_i)p(x_i^1) $$\n",
    "\n",
    "often used in two classes classification <br>\n",
    "more on likelihood function http://mathworld.wolfram.com/MaximumLikelihood.html <br>\n",
    "relationship between likelihood and least sqaure fit. \n",
    "\n",
    "**one predictor vs. more predictor**:\n",
    "the relationship between x1 and y will be different if x2 is added and is correlated with x1;\n",
    "\n",
    "** drawback **: classes are well seperated, result is not stable; if n is small, x distribution is normal, it is less stable than LDA <br>\n",
    "\n",
    "<h3> Linear Discriminant Analysis </h3>\n",
    "use Bayes therom for classification, how to estimate $f(x) =P(X|Y)P(Y)$ become key problem  <br>\n",
    "**assumption** : each density function of class follow a normal distribution, shared variance, class-specific mean<br>\n",
    "$$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$$\n",
    "calculate P(Y) from probability of records belongs to each class; assume $P(X|Y=k)$ to be normal distribution.\n",
    "$$ P(X|Y=k) = Normal(x,\\mu_k,\\sigma_k) $$ \n",
    "take log of $P(Y|X)$ (similar to Logistic regression) and and plug in x, so choose a class that make it the largest<br>\n",
    "assume $\\sigma_k$ is the same.\n",
    "estimate of $\\mu_k,\\sigma $ by for each class, calculate record in that class's average <br>\n",
    "In case we don't know the P(y=k), we estimate it by:\n",
    "$$ P(y=k) = n_k / n $$\n",
    "$$\\mu_k = \\frac{1}{n_k}\\sum_{y_i = k} x_i   $$\n",
    "$$\\sigma = \\frac{1}{n-K}\\sum_{k=1} \\sum_{y_i=k} (x - \\mu_k)^2  $$\n",
    "the decision boundary will be $x = (\\mu_1 + \\mu_2)/2$\n",
    "it is linear, as the log probability is linear to x <br>\n",
    "**maximum likelyhood**: P(x|parameters), is the maxmize the probablity of x by adjusted the statistical parameters. <br> \n",
    "\n",
    "<h3>multiple predictor of LDA </h3>\n",
    "multiple gaussian distribution, there are some correlation among each pair predictors;<br>\n",
    "**Higher P to n ratio**, higher possible for overfitting<br>\n",
    "\n",
    "<h3> confusion matrix   </h3>\n",
    "** error rate **  = wrong prediction / total prediction, it is about **total rate not class-spefic** <br> \n",
    "change the error rate of important class by change the threlshold of the posterier probability, \n",
    "so that the class-specific error rate can be improved <br> \n",
    "**class-specific**: this is very import than the total error rate <br>\n",
    "**ROC **: x is False positive rate, y is the True positive rate\n",
    "**AUC **: area under the ROC curve. larger the better.  <br>\n",
    "** true positive rate** is the senstiity, **false positive rate** is the specificity. <br>\n",
    "** negative** as null test, ** positive** as non-null test; \n",
    "**False positive**: Type I error, wrong rejection a true null hypothesis. ** False negative **: wrong retaining a false null hypothesis <br>\n",
    "\n",
    "<h3> ROC </h3>\n",
    "for comparion model parameters, similar to adjusted R^2 for regression <br>\n",
    "** sensitivity** TPR(y) = TP/Positive = TP/(TP+FN);**specificity** FPR(x) = FP/(FP+TN); using different threholds for logistic regression; calculate the ROC curve;  AUROC is the area under the ROC curve. the larger the better.<br> \n",
    "more details: https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it<br>\n",
    "\n",
    "<h3> </h3>\n",
    "for different impact on different, not just the probability matters,  define a loss function * probability \n",
    "lower the threlhod of some probability instead of 50%. <br>\n",
    "\n",
    "<h3> QDA </h3>\n",
    "assume each class has its own covariance matrix <br>\n",
    "x is quadratic function instead of linear function in term of log probability <br>\n",
    "\n",
    "<h3> comparison of different methods </h3>\n",
    "LDA less parameter to estimate, less flexible with lower vairance. when less data, LDA is prefered. <Br>\n",
    "logistic regression and LDA, the form is similar, different from their fitting producues;\n",
    "but LDA assume X follow normal distribution in every y<br>\n",
    "knn is total non-parametric approach, no assumption for the decision boundary, but don't tell us which predictor is important , it can capture the non-linear decision boundary <br>\n",
    "QDA serves a compromise between KNN and LDA using quadratic decision boundary <br>\n",
    "\n",
    "<h3> extension </h3>\n",
    "adding x^2 , x^3 to LDA to get between LDA and QDA. adding x^2 to logistic regression to polynomial regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Resampling</h2>\n",
    "estimation important statistics using resampling: ** test error ** use CV to evaluate model performance,\n",
    "**variance of model**, boostrap is used to measure the accuracy(uncertainty) of measure of parameter or a method <br>\n",
    "using validation set: **problem**, test error is highly variable; also less data for training, result overestimate test error <br>\n",
    "<h3>CV</h3>\n",
    "**k-fold**: random divide data into k folds, select one as validation set, use rest k-1 folds as training. select another as validation set and repeat for k times, calculate the average of k validation error <br> \n",
    "The location is important, the value is not that important <br>\n",
    "**classification** use **error rate** <br>\n",
    "<h3> boostrap </h3>\n",
    "simulating getting new samples sets without getting addtional samples by repeated resampling from original sampling sets; \n",
    "use **replacement**; calcualte the parameter $\\alpha$, then for all $\\alpha$, get std by :\n",
    "$$\\sqrt{ \\frac{1}{B-1}\\sum{(\\alpha_i - \\alpha_{mean}})^2 } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model selection </h3>\n",
    "modify linear-relationship to non-linear, but still using addictive model <br>\n",
    "**why**: prediction accucary: when p is large, even > n; linear-model have large variance; model interpretaion: remove irrelevant predictors; <br> \n",
    "**method**: subset selection: use only a fraction of predictors; shrinkage method: use all predictors but tend to shrink the coeffs, it can **reduce variance**, and **feature selection**; dimension reduction; projecting to a M space; <br>\n",
    "\n",
    "<h3> subset selection </h3>\n",
    "**algo 1 **, start from no predictor, select one from p by the low RSS, then select two from P by $p(p-1)/2$ using lowest RSS. then select k predictorrs from p follwing that; from these best model, using **C-V** to calculate AIC,BIC, adjusted R^2 to select the single best model, otherwise we will still end up all predictors <br>\n",
    "use **deviance**, -2 * maxmized log-likelihood; smaller, the better; <br>\n",
    "**drawback**: too computational cost, there are p space, choose either place predictor or not, so it is $2^p$ <br>\n",
    "\n",
    "<h3> how to calculate testing error</h3>\n",
    "RSS tends to underestimate testing error\n",
    "**indirect**: Cp,AIC, BIC, Adjusted $R^2$ <br>\n",
    "adding penelty taking into account variance of error (d is number of predictor, n is number of record)\n",
    "$$C_p = \\frac{1}{n}(RSS+2d\\sigma^2) $$ \n",
    "AIC: $$ AIC = \\frac{1}{n\\sigma^2}(RSS+2d\\sigma^2) $$\n",
    "BIC, place larger penalty for model with more variables: $$ BIC = \\frac{1}{n}(RSS+\\log(n)d\\sigma^2) $$\n",
    "Adjusted R2: $$ R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-d-1}$$\n",
    "**good and bad**: less computation cost, need to make assumption,indirect method <br>\n",
    "\n",
    "<h3>shrinkage method</h3>\n",
    "to reduce the amplitude of coeffs and thus reduce the variance; use cross-validation to select $\\lambda$ <br>\n",
    "expression( no penalty on $\\beta_0$) :\n",
    "$$ \\sum{e^2} + \\lambda \\sum_{k=1}^p{\\beta^2} $$\n",
    "usually we need to center the data of each col to have zero mean, so $\\beta_0 = \\sum(y_i)/n$ <br>\n",
    "\n",
    "**standardizing the predictors**, least sqaure fit not depend on scale, but the coeff determined by ridge regression, depend on $\\lambda$ as well as data scale. \n",
    "$$ xij = \\frac{xij}{std_j}  $$\n",
    "\n",
    "**good and bad**: reduce variance, computational cost less than subset selection model. bad: Include all P <br>\n",
    "\n",
    "**lasso**: better than ridge in term of interpretation. \n",
    "$$ \\sum{e^2} + \\lambda \\sum_{k=1}^p{|\\beta|} $$\n",
    "**why zero**: because rectangle shape, result in intersect at corner with some predictor to be zero. <br>\n",
    "** lasso vs ridge**, if small number of predictors (the **signal** predictor) have great effects, lasso is favored. also if concern about interpretation use lasso <br>\n",
    "lasso reduce the coeffs by minus, ridge reduce the coeffs by divide. so lasso will reduce some parameters to zeros. <br>\n",
    "\n",
    "**Bayesian interpretation**:\n",
    "error is normal distribution; further if $p(\\beta)$ is normal distribution, it is ridge regression, if it is double-exponential distribution, it is lasso solution <br>\n",
    "\n",
    "$$ f(x|\\mu,b) = \\frac{1}{2b}\\exp(-|x-\\mu|/b) $$\n",
    "\n",
    "<h3>Dimension reduction methods </h3>\n",
    "reduce dimension from p+1 to M+1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Tree model </h3>\n",
    "**Definition**:A decision tree is a schematic, tree-shaped diagram used to determine a course of action or show a statistical probability. Each branch of the decision tree represents a possible decision/ or value. <br>\n",
    "\n",
    "\n",
    "**math def**: seperate the x space into J area using rectangle decision boundary. use the average or mode of x variance in the region as prediction. The number of nodes is the number of split regions<br> \n",
    "**cost function**: $$ \\sum_j\\sum_{x_i\\in{R_j}} y_i - y  $$\n",
    "terminal node(leaf node), every region is terminal node. internal node: pointa along the tree for spliting the space <br>\n",
    "**algo**: binary tree; start from all region, try every predictor and split their region into two, selecting the best predictor and cutoff(decision boundary). \n",
    "Then split region into three following by selecting current best predictor and cutoff. stop when reach some threshold(number of point in this region is less than a value)<br> \n",
    "```python\n",
    "class Tree_node():\n",
    "    def __init__(predictor,cutoff):\n",
    "        node.cutoff = cutoff\n",
    "        node.predictor = predictor\n",
    "        node.left, node.right = None,None\n",
    "```\n",
    "main\n",
    "```python\n",
    "    if sum(points[j]) < certain_points\n",
    "    if depth > certain_depth \n",
    "        return \n",
    "    min(mse), return cuoff,x()\n",
    "    \n",
    "```\n",
    "**tree pruning**. smaller tree with few splits has less variance.  <br>\n",
    "change cost function to:  T is the total number of terminal nodes\n",
    "$$ \\sum_j^T\\sum_{x_i\\in{R_j}} (y_i - y)^2 + \\alpha|T|  $$\n",
    "$\\alpha$ is determined by CV <br>\n",
    "\n",
    "<h4> classification tree </h4>\n",
    "measure the purification <br>\n",
    "Gini index ($P_{mk}$, portion of training data in region m that assign to k )<br>\n",
    "$$ G = \\sum_{k=1}^K p_{mk}(1-P_{mk})$$\n",
    "cross entropy <br>\n",
    "$$ D = -\\sum_{k=1}^K p_{mk}\\log P_{mk} $$\n",
    "advantage： works best for non-linear relationship; easy interpretation; easier than linear model; mirro human decision-making; \n",
    "graphically. <br>\n",
    "disadvantage: low prediction accuracy. non-robust, senstive to the data <br>\n",
    "\n",
    "<h3> Bagging, random forests, boosting </h3>\n",
    "**bagging**: use boosstrap (repeated sample with replacement) to create many training sets, build seperate model; for regression, average the prediction outcome; for classification, use majority vote <br>\n",
    "large number of tree will not lead to overfitting <br>\n",
    "disadvantage: low interpretatbility <br>\n",
    "feature importance: decrease of RSS (Gini index) due to split on certain predictor. <br>\n",
    "**random forests** improve bagging using decorrelates tree. <br>\n",
    "algo: only m out of p predictors is allowed to split, avoid the dominance of strong predictor <br> \n",
    "**boost tree**: start with residual = yi, fit a tree using current residuals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> clustering method</h3>\n",
    "**DBSCAN**: Density-based spatial clustering of applications with noise : group points that are closely packed together(with many neighbors), outlier will be the lay in the lower density region <br>\n",
    "**Theory**:\n",
    "1. **Point reachable**: Point p is considered as a **core point**, if with defined a distance, it its neighbor number is larger than mini number of neighbors. These neighbor are reachable by P. \n",
    "2. **Path reachable through core points**: if p2 is reached by p1, p3 is reached by p2,... pk is reach by $p_{k-1}$, then pk is reachable by p1. All these reachable point are belongs to the same cluster.(connection through the core point).\n",
    "3.  All points not reachable from any other point( **core point**) are outliers. <br>\n",
    "\n",
    "** k mean clustering **:\n",
    "**metrics**: Calculate within cluster variation $W(C_k)$, minimize the summation of cluster variation $\\sum W(C_k) $ <br>\n",
    "cluster variation is summation of pair-wise Euclidean distance square.; Euclidean distance is the summation of difference of every feature. \n",
    "**algo**: \n",
    "1. Random assign a number from 1 to K to each observation. \n",
    "2. For each cluster, get centroid (mean of every feature), assign each observation of closest cluster centroid. keep iteration until no change of clustering. \n",
    "\n",
    "\n",
    "** hierarchical clustering**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generic Algorithm </h3>\n",
    "**Definition**: generate a bunch of \"answer candidates\" (**population**) and use some sort of feedback(**cost function**) to figure out how close the candidate is to optimal. Far-from-optimal candidates literally die and are never seen again. Close-to-optimal candidates combine with each other(**crossover**) and maybe mutate slightly(**mutation**)<br>\n",
    "**chromosomes vs gene**: chormosomes are the candidate or combination of parameter, gene are the single parameter, of basic subcollection of (or token) of the choromosmes. e.g. if goal is to find out the 'Hello World', then chormosomes are string, and gene are single character <br>\n",
    "**crossover vs mutation**: crossover exploit the potential better candidate from already good candidate; mutation is exploration by randomly introducing outside character to avoid local minimum. it also void stopping of evolution <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Deep learning </h3>\n",
    "extend linear model to nonlinear model by nonlinear transformation <br>\n",
    "$$ h = g(W^T x + c)$$\n",
    "**cost function** <br>\n",
    "learning conditional distribution : negative log-likelyhood/ cross-entropy <br>\n",
    "learning stats (like mean) instead of distribution: mean square error <br> \n",
    "** output ** <br>\n",
    "regression: \n",
    "classification: if binary, use sigmoid; if multi, use softmax\n",
    "\n",
    "<h3> Autoencoder </h3>\n",
    "learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. The **encoder** part learn a low dimensition representation of the input, and **decoder** part try to reconstruct the input from the encoder. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Algorithm</h2>\n",
    "**algo stepwise**: \n",
    "If a predictor has been selected, it will always be there. <br>\n",
    "**total number of model**, 1 + 1+2+3+...+p-1 = 1+ (p+1) * p/2 <br>\n",
    "main function\n",
    "```python\n",
    "Mo = 1\n",
    "visit = set()\n",
    "results= []\n",
    "select_features = []\n",
    "for k in range(p):\n",
    "   mini_rss = 99999\n",
    "   mini_pre = None\n",
    "   for predictor in predictors:\n",
    "       if preditor not in visit:\n",
    "           rss = get_rss(data, target, predictors, predictor, select_features)\n",
    "           if rss < mini_rss:\n",
    "               mini_rss = rss\n",
    "               mini_pre = predictor\n",
    "  visit.add(mini_pre)\n",
    "  select_features.append(mini_pre)\n",
    "```\n",
    "```python\n",
    "def get_rss(data, target, predictors, predictor, features)\n",
    "    all_features = list(feautres)\n",
    "    all_features.append(features)\n",
    "    iid = [i for i,e in enumerate(predictors) if e in all_features]\n",
    "    data_sub = data[:][iid]\n",
    "    target_sub = target[:][iid]\n",
    "    linear_model = sklearn.linearmodel()\n",
    "    linear_model.fit(data_sub, target_sub)\n",
    "    return linearmodel.rss \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> what is ML </h3>\n",
    "there is pattern, cannot solve by mathematically, you have data <br>\n",
    "\n",
    "<h3> big data </h3>\n",
    "how to do when data is too big; randomly sampling data; \n",
    "**reduce dimension**: numerical data use covariance matrix, categorical data using chi-square test. (test if two follow same distribution) ; \n",
    "use **PCA** to reduce dimension; **on-line learning** when apply to optimization, e.g. recursive least squares, like stochastic gradient descent;\n",
    "**domain knowledge**; <br>\n",
    "\n",
    "<h3> imputation </h3>\n",
    "** time series**: forward feeding;  \n",
    "\n",
    "<h3> high dimension</h3>\n",
    "use lasso or ridge,\n",
    "\n",
    "<h3> feature selection </h3>\n",
    "remove correlated features; best subset selection; lasso; random forest to get feature important; **linear regression and select variable based on P values.** <br>\n",
    "\n",
    "<h3> covariance </h3>\n",
    "correlation is the standardized form of covariance;\n",
    "use **ANCOVA** to capture correlation among continous and categorical more http://www.biostat.umn.edu/~lynn/ph7406/notes/ancova.pdf <br>\n",
    "\n",
    "<h3> high variance </h3>\n",
    "use **bagging**, generate a set of model, combine model results using averaging or major voting;\n",
    "**regulization**, penalize higher model coeffs; (ridge)\n",
    "**reduce features**: findout the signal features.(lasso/best subset selection)\n",
    "**cross-validation**: find the best tuninig parameters <br>\n",
    "\n",
    "<h3>Cross- validation </h3>\n",
    "**time series**: use forward chaining: train[1,2] test[3], train[1,2,3] test[4] and so on; <br>\n",
    "\n",
    "\n",
    "<h3> linear regression</h3>\n",
    "**collineality**: covariance matrix or variance inflation factor; if you want to retain all features, use regulization;<br>\n",
    "\n",
    "<h3> Classification </h3>\n",
    "1. classification problem;  classifiers are more sensitive to detecting the majority class <br> \n",
    "undersampling, oversampling, smote ; more http://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation <br> \n",
    "**undersampling**: select n samples at random from the majority class, where n is the number of samples for the minority class <br>\n",
    "**oversampling**: may cause overfit if use CV later (validation contained duplicated sampling in the training due to increase of minor group sample size); correct way to leave one in minor group as validate set, do the rest for oversampling; <br>\n",
    "**SMOTE**: https://www.jair.org/media/953/live-953-2037-jair.pdf <br>\n",
    "**alter prediction threshold** and find a optimal one from AUC_ROC curve <br>\n",
    "**assign higher weights to minor group**, similar to have more duplicated minor group samples <br>\n",
    "2. true positive rate = recall = senstivity. \n",
    "3. ** label encoding vs one-hot encoding**: LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2]. one hot extend to vector with len = number of the category. put one item as one the rest as zero. \n",
    "\n",
    "<h3> PCA </h3>\n",
    "rotation is important, is maximize difference between variance captured by the components<br>\n",
    "if two variables are correlated, you should drop one; explained variance will be inflated <br>\n",
    "\n",
    "<h3>regulization</h3>\n",
    "when there is no strong features, they all have small/medium effects, use ridge.<br>\n",
    "\n",
    "<h3> naive Bayes </h3>\n",
    "assume all features are equall important and independent; \n",
    "**likelihood**: probability of classifying a observation as a certain class with given some probabilitic model parameters. \n",
    "**margin likelihood**: simply P(x), integration of all the likelihood; \n",
    "\n",
    "<h3> Tree model </h3>\n",
    "**linear**: time series posses linearity, not suitable for tree model if the linearity is strong <br>\n",
    "**strong correlated** for ensembling model, if they are strong correlated, the result will not be good. that's why we use random forest to have uncorrelated model <br>\n",
    "** random forest vs GBM **: randome forest using bagging, GBM using boosting; random forest reduce variance only; GBM reduce both variance and bias; <br>\n",
    "\n",
    "** how to calculate GINI** : <br>\n",
    "** overfitting of random forest **: when use too large number of trees, overfitting can happen, or you need to change the m size; finally use C-V to check. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>possible projects</h2>\n",
    "\n",
    "<h3> recommendation </h3> \n",
    "https://medium.com/ai-society/a-concise-recommender-systems-tutorial-fa40d5a9c0fa <br> \n",
    "collaborative filtering: find similar users for target user, who share the same rating patterns. Use the ratings from those similar users to predict item rating for the target user; similar to **KNN**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
