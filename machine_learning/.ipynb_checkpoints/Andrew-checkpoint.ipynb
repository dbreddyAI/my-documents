{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic notation <br>\n",
    "${x_j}^{(i)}$ means, ith record, jth feature <br>\n",
    "Two things about gradient descent:1 feature scaling or mean normalization; 2 learning rate<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient decent\n",
    "$\\theta$ is (m+1,1) vector, x is (m+1,1) vector, \n",
    "$$h = \\sum_{i=0}^{m} \\theta_i x_i = \\theta ^T x $$\n",
    "loss function \n",
    "$$ L = 1/2 \\sum_{j=0}^{n-1}(f_j(w) - y_j)^2$$\n",
    "gradient decent\n",
    "$$ w_i = w_i - l \\frac{\\partial L}{\\partial (w_k)} $$\n",
    "where l is the learning rate. <br>\n",
    "\n",
    "gradient of loss function if only one sample.\n",
    "$$\\frac{\\partial L}{\\partial (w_k)} = (f_j - y_j)\\frac{\\partial}{\\partial (w_k)} \\sum_{i=0}^{m-1}w_ix_i  $$\n",
    "$$ = (f_j - y_j)x_k  $$\n",
    "If using all samples information(batch gradient)\n",
    "$$ w_k = w_k - l \\sum_{i=0}^{m-1}(f_i -y_i)x_k   $$\n",
    "If only one sample. for every sample i, the weight get updated\n",
    "$$ w_k = w_k - l (f_i -y_i)x_k   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale <br>\n",
    "1. **feature scaling**: <br>\n",
    "$$ \\frac{x}{max - min}$$\n",
    "different features take similar scale (about -1<= x <=1). It helps gradient descent converge quicker. (e.g. the shape of the loss function contour will be regular shape if they have similar scale)<br>\n",
    "2. **mean normalization**: <br>\n",
    "$$ \\frac{x- \\mu}{\\sigma} $$ or $$ \\frac{x- \\mu}{max - min} $$ <br>\n",
    "3. it is important for especially **polynomial model**. \n",
    "\n",
    "**learning rate**<br>\n",
    "checking if gradient decent works? plot cost function vs iteration\n",
    "1. **observation**:for i iteration, got the weights, plug in cost function, then see plot of the cost function vs iteration to check if it works. \n",
    "2. **auto stop**, calculate the change of cost function, if it is less than the threshold, then stop \n",
    "3. if it goes up, it means not converging, it also indicate of **too large learning rate**.  \n",
    "4. if learning rate is small enough, J can converge. but too small learning rate takes too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Logistic regression </h3>\n",
    "adding sigmoid function \n",
    "$$z = \\theta ^T x $$ \n",
    "$$h(z) = \\frac{1}{1+ e^{-z}}$$\n",
    "h(z) is from 0 to 1, very similar to probability p(z). <br>\n",
    "\n",
    "**Decision boundary**, setting thredhold to be 0.5, which means h(z) > 0.5, we classify to be Class 1. This is equal to say we classify to be Class 1 when z > 0 or $\\theta ^ T x > 0 $. The hyperplane $\\theta^Tx=0$ will form a linear decision boundary on on the   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
